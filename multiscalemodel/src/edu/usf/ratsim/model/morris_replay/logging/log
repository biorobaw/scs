Morris Replay Model Log file


6/26/2018
-Modified TesselatedPlaceCEllLayer
   -Class no longer has access to robot
   -Position is no longer taken from the robot but given as an input to the module
-Restoring snapshots
   -modified format in which binary files are saved
   -deleted ModelSave interface, added save and load functions to class Model
   -modified xml file, experiment and episode to load model if defined in control variables on xml file
   -RESTORED
   
07/02/2018
  -Changing how V and Q are updated
      before: always update V and Q
      now:    update only if action taken was optimal action according to policy or if error > 0
      *Done: created "MaxModule" and "AreEqual" module, also added extra input to the calculation of deltaError
      Result: Policy "somewhat" learns the path but rapidly converges to trajectories that get stuck in a loop ("circle")
      Currently studying reasons: need to improve debugging tools to observe how the system evolves
      next steps: improve V drawers, create Q drawer, create W drawer, also should reward similar actions as well since it is open maze
      	* V drawer improved a bit
      	Issues found: State generalization may rewarding/penalize actions which may not make sense, view image on "issue1" of issues.pptx
      	
07/03/2018
	-Analyzing issues 1 and 2 found on 07/02/2018: is the problem how V and Q are updated?
  	-Math analysis shows implemented equations have a tendency to either keep increasing or keep decreasing V, see equations in issue3 in  "issues.pptx" 
  	-Making changes specified in "issues.pptx : Issue 3 – modifying the model to solve issue"
  	     *Updated Awake and Asleep models
  	     *Modified ActorCriticDeltaError so that it now receives two inputs instead of using memory (which it was not ok)
  	     *Note: before update: module updateVQ executed before choosing an action, now they are inverted, thus the dependency was also inverted to update old action
  	     *Done with modification, error solved, must check whether learning correctly now.
  	
07/04/2018
	-Analysing results after changes made on 07/03/2018: does the model learn correctly now?
	-Issue: the code is inefficient, each run takes too long -> must optimize
	-Profiler tool shows:
			Main              				296.008    
			PCTransitionMatrixUpdater  		 40
			DrawPanel.paint() 				167.239
			    VDrawer       				109.993
			    PCDrawer       				 49.857
			    PathDrawer      			  2.71
			Plotter							  4.0
		Most time utilized by drawing all PCs and updating matrix W
		
		
07/05/2018
	-Reviewing performance issues
		-Display: simplifying repaint mechanism, updating so that only a single synchronization point with simulation
			-Done simplification
		-TODO: 
			-change tesselated place cell layer to use a reduced matrix (create that matrix)
			-improve drawing functions
			
07/06/2018
	-Creating SingleBlockMatrix and SingleBlockMatrixPort
		-Done
		-Model updated, model sped up 100x, render time sped up 2x, rendering times can only improved in screens with refresh rate of 60x
		-Screen refresh rates in ms: 
			30 fps = 33ms
			60 fps = 16.6ms
			120fps = 8.3ms
			240fps = 
	-Running in async mode may cause issues with some drawers which require the history of a variable (example draw paths), fixing that...
		
		
07/09/2018
	-Fixed path drawer in async mode
	-Testing current solution computation efficiency - sufficiently good, might require		
	
07/10/2018
	- Changing how Q is updating. Before we used the following formula for both V and Q:
		error = r_t  + gamma * V(s_{t+1})  -   V(s_t)    if action was optimal or error > 0
		else:  error = 0;
	  After update, V uses the same error, but Q uses the following formula for the error:
	    errorQ = r_t + gamma * V(s_{t+1}) - Q(s_t,a_t)
	 Change finished
	-Modified WTable so that it gets erased each episode
	-Changed replay episodes from 200 to 50 to 0
	-Changing affordance bias
	-Tested several learning rates:
		0.6 
		0.30
		0.15	
		0.06
		with 0.6 reward is propagated all the way back to initial position
		with 0.3 it is only propagated between 1/2 and 1/3 of the way back to initial position
		biases allow the robot to converge fast any way
		Also, lower learn rates are more stable since value functions update slower
	-ISSUE 4 : Found issue with radial basis functions, view issue 4 on "issues.pptx". Problem is that the lack of HD cells make what should be different states indistinguishable.
	Reason is radial basis function generalize the policy to whole area, so if moving to the right from the left of the food is a good action, so will it be if the robot is placed slightly to the right of the food.	
	-ISSUE 5 : Matrix update equation "atan( average(P_i)*delta(P_j) )" increase strength between cells even when moving perpendicular to those cells		
		
07/11/2018
	-Fixing load function - seems like random streams are not the same - fixed: random action selection was storing the random stream before being loaded